{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb2efc6",
   "metadata": {},
   "source": [
    "# Xenium Human Lung Preview: Data Import & Matrix Construction\n",
    "\n",
    "This notebook processes the raw 10x Genomics *Xenium Human Lung Preview* (FFPE, non-diseased) dataset.  \n",
    "It ingests the original **cells** and **transcripts** tables, detects spatial coordinates, and builds a sparse **cell × gene matrix** in the form of an `AnnData` object.  \n",
    "\n",
    "**Workflow steps:**\n",
    "1. **Environment setup** – imports, paths, plotting defaults, random seed.  \n",
    "2. **Cell metadata** – load `cells.parquet`, decode strings, detect XY, prepare `obs`.  \n",
    "3. **Gene features** – parse `features.tsv.gz` to exclude non-gene controls.  \n",
    "4. **Matrix construction** – two-pass build of sparse counts from `transcripts.parquet` (QV ≥ 20, in-cell, genes only).  \n",
    "5. **AnnData creation** – assemble `X`, `obs`, `var`, QC metrics, and spatial coordinates.  \n",
    "6. **Save outputs** – write `adata_raw.h5ad` and a QC metrics CSV for downstream analysis.  \n",
    "\n",
    "> **Note:** This notebook ends by saving `adata_raw.h5ad`.  \n",
    "> All normalization, clustering, visualization, and marker analysis are performed in **Notebook 2 (02_xenium_downstream.ipynb)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d1d23-2e50-43d2-97f2-bd7186428870",
   "metadata": {},
   "source": [
    "## Environment setup and imports\n",
    "\n",
    "This section loads all required Python packages, sets a fixed random seed for reproducibility, configures paths for data, results, and figures, and applies plotting defaults. It also ensures the expected Xenium dataset folder exists before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557b7ae9-52ce-4b6d-b35f-d930a4d0d689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juliors/.local/share/mamba/envs/xenium/lib/python3.11/site-packages/dask/dataframe/__init__.py:31: FutureWarning: The legacy Dask DataFrame implementation is deprecated and will be removed in a future version. Set the configuration option `dataframe.query-planning` to `True` or None to enable the new Dask Dataframe implementation and silence this warning.\n",
      "  warnings.warn(\n",
      "/home/juliors/.local/share/mamba/envs/xenium/lib/python3.11/site-packages/xarray_schema/__init__.py:1: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "/home/juliors/.local/share/mamba/envs/xenium/lib/python3.11/site-packages/anndata/__init__.py:44: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
      "  return module_get_attr_redirect(attr_name, deprecated_mapping=_DEPRECATED)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Standard library imports\n",
    "# =========================\n",
    "from __future__ import annotations  \n",
    "import gc\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pathlib \n",
    "\n",
    "# ======================\n",
    "# Third-party libraries\n",
    "# ======================\n",
    "import anndata as ad\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import squidpy as sq\n",
    "from matplotlib.colors import LogNorm, PowerNorm\n",
    "from scipy.sparse import coo_matrix, csr_matrix, issparse\n",
    "\n",
    "\n",
    "\n",
    "# =====================================\n",
    "# Set global seed for reproducibility\n",
    "# =====================================\n",
    "import random\n",
    "SEED = 123\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1275ce53-a1d7-4d02-a015-54eeddbf8583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using dataset folder: /home/juliors/Documents/SPATIAL-OMICS/xenium-raw-data-analysis-workflow/data/unpacked/xenium_preview_human_non_diseased_lung_with_add_on_ffpe\n"
     ]
    }
   ],
   "source": [
    "# Optional: quiet a dask warning some packages emit\n",
    "try:\n",
    "    import dask\n",
    "    dask.config.set({\"dataframe.query-planning\": True})\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Paths (assumes this notebook lives in repo/notebooks or repo/)\n",
    "ROOT = pathlib.Path.cwd()\n",
    "if ROOT.name == \"notebooks\":\n",
    "    ROOT = ROOT.parents[0]\n",
    "DATA = ROOT / \"data\"\n",
    "UNPACKED = DATA / \"unpacked\"\n",
    "RESULTS = ROOT / \"results\"; RESULTS.mkdir(exist_ok=True)\n",
    "FIGS = RESULTS / \"figures\"; FIGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plot look\n",
    "sc.set_figure_params(dpi=120, frameon=False)\n",
    "sns.set_context(\"talk\", font_scale=0.9)\n",
    "\n",
    "# Find the unpacked dataset folder (created by your unpack script)\n",
    "FOLDER = UNPACKED / \"xenium_preview_human_non_diseased_lung_with_add_on_ffpe\"\n",
    "assert FOLDER.exists(), f\"Expected folder not found: {FOLDER}\"\n",
    "print(f\"[INFO] Using dataset folder: {FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e6eca-ea33-445b-8c6d-e6170ad8ea28",
   "metadata": {},
   "source": [
    "## Load cell coordinates and build cell × gene matrix\n",
    "\n",
    "This section:\n",
    "\n",
    "1. **Loads cell metadata** from `cells.parquet`, decoding byte strings, detecting XY coordinate columns, and keeping key spatial/shape attributes.\n",
    "2. **Generates a quick spatial density plot** (black theme, light grid) to verify tissue coverage.\n",
    "3. **Streams the transcript data** from `transcripts.parquet` in two passes:\n",
    "   - **Pass 1**: inventory of genes meeting quality and control filters.\n",
    "   - **Pass 2**: aggregate counts per `(cell_id, gene)` pair into a sparse matrix.\n",
    "4. **Constructs an AnnData object** with:\n",
    "   - Raw counts in `.layers[\"raw_counts\"]`\n",
    "   - QC metrics (`total_counts`, `n_genes_by_counts`)\n",
    "   - Spatial coordinates in `.obsm[\"spatial\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a1fac-7660-4b84-851a-0cac14a95fca",
   "metadata": {},
   "source": [
    "## Load and visualize cell metadata\n",
    "\n",
    "Read `cells.parquet`, decode text columns, detect and standardize XY coordinates, and keep essential metadata for downstream analysis.  \n",
    "Generates a quick 2D density plot (dark theme) to check spatial coverage of all cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd038aa4-025c-40c2-b874-022d36ada07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOAD] cells.parquet → 295,883 cells; coords = (x_centroid, y_centroid)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the cells table (arrow→pandas)\n",
    "cells_path = FOLDER / \"cells.parquet\"\n",
    "assert cells_path.exists(), f\"Missing {cells_path}\"\n",
    "cells_df = pq.read_table(cells_path).to_pandas()\n",
    "\n",
    "# Decode byte columns to strings (10x often stores 'binary' columns)\n",
    "for col in cells_df.columns:\n",
    "    if cells_df[col].dtype == object:\n",
    "        cells_df[col] = cells_df[col].apply(\n",
    "            lambda x: x.decode(\"utf-8\", \"ignore\") if isinstance(x, (bytes, bytearray)) else x\n",
    "        )\n",
    "\n",
    "# Ensure we have cell IDs\n",
    "assert \"cell_id\" in cells_df.columns, f\"'cell_id' missing; saw: {cells_df.columns.tolist()}\"\n",
    "\n",
    "# Robust XY detection (handle multiple historical names)\n",
    "lower_to_orig = {c.lower(): c for c in cells_df.columns}\n",
    "x_candidates = [\"x_location\",\"x_centroid\",\"center_x\",\"x\"]\n",
    "y_candidates = [\"y_location\",\"y_centroid\",\"center_y\",\"y\"]\n",
    "\n",
    "x_col = next((lower_to_orig[c] for c in x_candidates if c in lower_to_orig), None)\n",
    "y_col = next((lower_to_orig[c] for c in y_candidates if c in lower_to_orig), None)\n",
    "assert x_col and y_col, f\"Could not find XY in cells.parquet. Columns: {cells_df.columns.tolist()}\"\n",
    "\n",
    "# Standardize names for downstream tools\n",
    "cells_df = cells_df.rename(columns={x_col: \"x_location\", y_col: \"y_location\"})\n",
    "cells_df[\"x_location\"] = pd.to_numeric(cells_df[\"x_location\"], errors=\"coerce\")\n",
    "cells_df[\"y_location\"] = pd.to_numeric(cells_df[\"y_location\"], errors=\"coerce\")\n",
    "\n",
    "# Keep a clean obs table (you can add more columns if you like)\n",
    "keep_cols = [\"cell_id\",\"x_location\",\"y_location\"]\n",
    "for extra in [\"fov_name\",\"area\",\"nucleus_area\"]:\n",
    "    if extra in cells_df.columns: keep_cols.append(extra)\n",
    "\n",
    "cells_df = cells_df[keep_cols].set_index(\"cell_id\", drop=False).sort_index()\n",
    "print(f\"[LOAD] cells.parquet → {len(cells_df):,} cells; coords = ({x_col}, {y_col})\")\n",
    "\n",
    "# Quick sanity figure — black theme\n",
    "plt.style.use(\"dark_background\")\n",
    "fig, ax = plt.subplots(figsize=(5.6, 5.6))\n",
    "\n",
    "# Plot density\n",
    "h = ax.hist2d(\n",
    "    cells_df[\"x_location\"], cells_df[\"y_location\"], bins=300, cmap=\"viridis\"\n",
    ")\n",
    "\n",
    "ax.invert_yaxis()\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_title(\"Spatial: cell density (all cells)\", color=\"white\")\n",
    "\n",
    "# Light gray, thin grid every ~3000 units\n",
    "ax.set_xticks(range(0, int(cells_df[\"x_location\"].max()), 3000))\n",
    "ax.set_yticks(range(0, int(cells_df[\"y_location\"].max()), 3000))\n",
    "ax.grid(color=\"gray\", alpha=0.3, linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS / \"00_spatial_density_all_cells_black.png\", dpi=200)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e474c1da-2394-4720-a079-f67bea2cb353",
   "metadata": {},
   "source": [
    "## Build filtered cell × gene count matrix\n",
    "\n",
    "Generate a sparse cell–gene matrix from `transcripts.parquet` (QV ≥ 20, in-cell, genes only, controls excluded), matching the behavior of 10x’s `cell_feature_matrix`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3b04c1e-f994-4cc5-be99-0d8b3ff402a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pass1] batches=4 (genes so far: 392)\n",
      "[pass1] batches=8 (genes so far: 392)\n",
      "[pass1] batches=12 (genes so far: 392)\n",
      "[pass1] batches=16 (genes so far: 392)\n",
      "[pass1] batches=20 (genes so far: 392)\n",
      "[pass1] batches=24 (genes so far: 392)\n",
      "[pass1] batches=28 (genes so far: 392)\n",
      "[pass1] unique genes: 392\n",
      "[SHAPE] target matrix: 295,883 cells × 392 genes\n",
      "[pass2] batches=  4  nnz=2,054,020\n",
      "[pass2] batches=  8  nnz=4,096,753\n",
      "[pass2] batches= 12  nnz=6,115,699\n",
      "[pass2] batches= 16  nnz=8,086,845\n",
      "[pass2] batches= 20  nnz=10,110,691\n",
      "[pass2] batches= 24  nnz=12,180,785\n",
      "[pass2] batches= 28  nnz=14,240,040\n",
      "[BUILD] sparse matrix complete: shape=(295883, 392), nnz=14,544,817\n"
     ]
    }
   ],
   "source": [
    "# === Build a cell × gene matrix from transcripts.parquet (QV≥20, genes-only, in-cell) ===\n",
    "# Keeps behavior consistent with 10x cell_feature_matrix.  (QV≥20; exclude controls)\n",
    "\n",
    "# ---- paths ----\n",
    "tx_path = FOLDER / \"transcripts.parquet\"\n",
    "cfm_dir = FOLDER / \"cell_feature_matrix\"           # contains features.tsv.gz\n",
    "feat_tsv = cfm_dir / \"features.tsv.gz\"\n",
    "assert tx_path.exists(), f\"Missing {tx_path}\"\n",
    "assert feat_tsv.exists(), f\"Missing {feat_tsv} (needed to drop controls).\"\n",
    "\n",
    "# ---- load 'Gene Expression' feature names from features.tsv.gz ----\n",
    "# columns: [ensembl_id, feature_name, feature_type]\n",
    "with gzip.open(feat_tsv, \"rt\") as fh:\n",
    "    feats = pd.read_csv(fh, sep=\"\\t\", header=None, names=[\"ensembl_id\",\"feature_name\",\"feature_type\"])\n",
    "gene_names = pd.Index(feats.loc[feats[\"feature_type\"]==\"Gene Expression\",\"feature_name\"].astype(str).unique())\n",
    "gene_name_set = set(gene_names)\n",
    "\n",
    "# ---- dataset & schema ----\n",
    "dataset = ds.dataset(tx_path)\n",
    "cols = {c.lower(): c for c in dataset.schema.names}\n",
    "GENE_COL = next((cols[k] for k in [\"feature_name\",\"gene\",\"gene_name\",\"target\"] if k in cols), None)\n",
    "CELL_COL = cols.get(\"cell_id\")\n",
    "QV_COL   = next((cols[k] for k in [\"qv\",\"quality\",\"q\"] if k in cols), None)\n",
    "assert GENE_COL and CELL_COL and QV_COL, f\"Missing required columns; saw: {list(cols.values())}\"\n",
    "\n",
    "def bytes_to_str(s):\n",
    "    if s.dtype == object:\n",
    "        return s.apply(lambda x: x.decode(\"utf-8\",\"ignore\") if isinstance(x,(bytes,bytearray)) else x)\n",
    "    return s\n",
    "\n",
    "# ---- Pass 1: gene inventory under filters ----\n",
    "PASS1_BATCH = 5_000_000\n",
    "gene_totals = {}\n",
    "\n",
    "need_p1 = [GENE_COL, CELL_COL, QV_COL]\n",
    "for i, batch in enumerate(dataset.to_batches(columns=need_p1, batch_size=PASS1_BATCH), start=1):\n",
    "    df = batch.to_pandas()\n",
    "\n",
    "    # normalize dtypes\n",
    "    df[GENE_COL] = bytes_to_str(df[GENE_COL]).astype(str)\n",
    "    # CRITICAL: make cell_id dtype match cells_df.index (string is safest)\n",
    "    df[CELL_COL] = df[CELL_COL].astype(str)\n",
    "\n",
    "    # filters: QV≥20, in known cells, genes only (exclude controls)\n",
    "    df = df[(df[QV_COL] >= 20) & (df[CELL_COL].isin(cells_df.index.astype(str))) & (df[GENE_COL].isin(gene_name_set))]\n",
    "    if df.empty:\n",
    "        del df; gc.collect(); continue\n",
    "\n",
    "    vc = df[GENE_COL].value_counts()\n",
    "    for g, n in vc.items():\n",
    "        gene_totals[g] = gene_totals.get(g, 0) + int(n)\n",
    "\n",
    "    if i % 4 == 0:\n",
    "        print(f\"[pass1] batches={i} (genes so far: {len(gene_totals):,})\")\n",
    "    del df, vc\n",
    "    gc.collect()\n",
    "\n",
    "genes = pd.Index(sorted(gene_totals, key=gene_totals.get, reverse=True), name=\"gene\")\n",
    "gene_to_col = {g:i for i, g in enumerate(genes)}\n",
    "print(f\"[pass1] unique genes: {len(genes):,}\")\n",
    "\n",
    "# ---- allocate target matrix ----\n",
    "ordered_cell_ids = cells_df.index.astype(str).tolist()  # ensure string\n",
    "cell_to_row = {cid:i for i, cid in enumerate(ordered_cell_ids)}\n",
    "n_cells, n_genes = len(ordered_cell_ids), len(genes)\n",
    "X = csr_matrix((n_cells, n_genes), dtype=np.int32)\n",
    "print(f\"[SHAPE] target matrix: {n_cells:,} cells × {n_genes:,} genes\")\n",
    "\n",
    "# ---- Pass 2: aggregate (cell_id, gene) counts with same filters ----\n",
    "PASS2_BATCH = 2_000_000\n",
    "need_p2 = [CELL_COL, GENE_COL, QV_COL]\n",
    "\n",
    "for i, batch in enumerate(dataset.to_batches(columns=need_p2, batch_size=PASS2_BATCH), start=1):\n",
    "    df = batch.to_pandas()\n",
    "\n",
    "    df[GENE_COL] = bytes_to_str(df[GENE_COL]).astype(str)\n",
    "    df[CELL_COL] = df[CELL_COL].astype(str)\n",
    "\n",
    "    df = df[(df[QV_COL] >= 20) &\n",
    "            (df[CELL_COL].isin(cell_to_row)) &               # faster than isin(cells_df.index) now\n",
    "            (df[GENE_COL].isin(gene_name_set))]\n",
    "    if df.empty:\n",
    "        del df; gc.collect(); continue\n",
    "\n",
    "    grp = df.groupby([CELL_COL, GENE_COL]).size().astype(np.int32)\n",
    "    del df\n",
    "\n",
    "    rows, cols_, data = [], [], []\n",
    "    ar, ac, av = rows.append, cols_.append, data.append\n",
    "    for (cid, g), n in grp.items():\n",
    "        r = cell_to_row.get(cid)\n",
    "        c = gene_to_col.get(g)\n",
    "        if r is not None and c is not None:\n",
    "            ar(r); ac(c); av(int(n))\n",
    "    del grp\n",
    "\n",
    "    if rows:\n",
    "        coo = coo_matrix((data, (rows, cols_)), shape=(n_cells, n_genes), dtype=np.int32).tocsr()\n",
    "        X += coo\n",
    "        del coo, rows, cols_, data\n",
    "\n",
    "    if i % 4 == 0:\n",
    "        print(f\"[pass2] batches={i:>3}  nnz={X.nnz:,}\")\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"[BUILD] sparse matrix complete: shape={X.shape}, nnz={X.nnz:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e2519-372d-4d7e-8988-ea72c41c3874",
   "metadata": {},
   "source": [
    "## Create and Save AnnData object with QC metrics\n",
    "\n",
    "Initialize an `AnnData` object from the cell–gene matrix, store raw counts, compute basic QC stats, and add spatial coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0cef22f-7360-4ebe-b56f-3dfbc9ca45de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ANNData] 295,883 cells × 392 genes (nnz=14,544,817)\n",
      "[SAVE] wrote AnnData with 295,883 cells × 392 genes → /home/juliors/Documents/SPATIAL-OMICS/xenium-raw-data-analysis-workflow/results/adata_raw.h5ad\n"
     ]
    }
   ],
   "source": [
    "adata = ad.AnnData(\n",
    "    X=X,                                       # sparse counts\n",
    "    obs=cells_df.loc[ordered_cell_ids],        # row metadata (cells)\n",
    "    var=pd.DataFrame(index=genes),             # col metadata (genes)\n",
    ")\n",
    "\n",
    "# Keep raw counts before normalization\n",
    "adata.layers[\"raw_counts\"] = adata.X.copy()\n",
    "\n",
    "# QC metrics and spatial coordinates\n",
    "adata.obs[\"total_counts\"] = np.asarray(adata.X.sum(axis=1)).ravel()\n",
    "adata.obs[\"n_genes_by_counts\"] = np.asarray((adata.X > 0).sum(axis=1)).ravel()\n",
    "adata.obsm[\"spatial\"] = adata.obs[[\"x_location\",\"y_location\"]].to_numpy()\n",
    "\n",
    "print(f\"[ANNData] {adata.n_obs:,} cells × {adata.n_vars:,} genes (nnz={adata.X.nnz:,})\")\n",
    "\n",
    "# --- Save handoff objects ---\n",
    "\n",
    "# Save AnnData with raw counts + QC metrics\n",
    "adata.write_h5ad(RESULTS / \"adata_raw.h5ad\", compression=\"lzf\")\n",
    "\n",
    "# Save a quick QC table (optional, lightweight summary)\n",
    "adata.obs[[\"total_counts\",\"n_genes_by_counts\"]].to_csv(\n",
    "    RESULTS / \"qc_metrics.csv\", index=True\n",
    ")\n",
    "\n",
    "print(f\"[SAVE] wrote AnnData with {adata.n_obs:,} cells × {adata.n_vars:,} genes → {RESULTS/'adata_raw.h5ad'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (xenium 3.11)",
   "language": "python",
   "name": "xenium-311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
